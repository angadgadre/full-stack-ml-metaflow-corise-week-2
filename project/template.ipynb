{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we‚Äôll be using the [Women's Ecommerce Clothing Reviews Dataset from Kaggle](https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews), which Kaggle states is a \n",
    "\n",
    "> dataset revolving around the reviews written by customers. Its nine supportive features offer a great environment to parse out the text through its multiple dimensions. Because this is real commercial data, it has been anonymized, and references to the company in the review text and body have been replaced with ‚Äúretailer‚Äù. \n",
    "\n",
    "The machine learning task will be sentiment analysis, classifying each review as having positive or negative sentiment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Training and Evaluating Sentiment Analysis Models Using Metaflow\n",
    "\n",
    "In this task, you'll use Metaflow to build two machine learning models for sentiment analysis: a baseline *\"majority class\"* classifier and your own custom model. You'll then train both models in parallel and experiment with different hyperparameters to optimize their performance. Finally, you'll use this notebook and the Metaflow Client API to analyze the results of your different models and hyperparameters. Here's what you'll need to do:\n",
    "\n",
    "### Step 1: Build the Workflows\n",
    "The first step in this task is to build the workflow(s) for sentiment analysis using the Metaflow framework. Start by creating a new flow in Metaflow and implementing the baseline *\"majority class\"* classifier. Then, build your own custom classifier using techniques you learned in Week 1, or any [helpful resources](https://outerbounds.com/docs/nlp-tutorial-L2/) you'd like. For your custom model, be sure to include steps for data preprocessing, model training, and evaluation.\n",
    "\n",
    "### Step 2: Train Both Models in Parallel\n",
    "Once you've built your models, the next step is to train both models in parallel using the Metaflow framework. Use Metaflow to run both training jobs in parallel steps. If you get stuck, you may want to review the [FlowSpec branching documentation](https://docs.metaflow.org/metaflow/basics#branch).\n",
    "\n",
    "### Step 3: Experiment with Hyperparameters\n",
    "After you've trained both models in parallel, the next step is to experiment with different hyperparameters to optimize their performance. Try different values for hyperparameters such as learning rate, batch size, and number of epochs, and record the results for each combination of hyperparameters as Data Artifacts in Metaflow.\n",
    "\n",
    "### Step 4: Analyze the Results\n",
    "Finally, use this notebook and the Metaflow Client API to analyze the results of your different models and hyperparameters. Create visualizations to compare the performance of the two models and identify the best hyperparameters for each one.\n",
    "\n",
    "By completing this task, you'll gain experience working with the Metaflow framework and learn how to build and optimize machine learning workflows for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from termcolor import colored\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "\n",
    "# You can style your plots here, but it is not part of the project.\n",
    "YELLOW = '#FFBC00'\n",
    "GREEN = '#37795D'\n",
    "PURPLE = '#5460C0'\n",
    "BACKGROUND = '#F4EBE6'\n",
    "colors = [GREEN, PURPLE]\n",
    "custom_params = {\n",
    "    'axes.spines.right': False, 'axes.spines.top': False,\n",
    "    'axes.facecolor':BACKGROUND, 'figure.facecolor': BACKGROUND, \n",
    "    'figure.figsize':(8, 8)\n",
    "}\n",
    "sns_palette = sns.color_palette(colors, len(colors))\n",
    "sns.set_theme(style='ticks', rc=custom_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Initmates</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>Intimates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age Title                                        Review Text  \\\n",
       "0          767   33   NaN  Absolutely wonderful - silky and sexy and comf...   \n",
       "1         1080   34   NaN  Love this dress!  it's sooo pretty.  i happene...   \n",
       "\n",
       "   Rating  Recommended IND  Positive Feedback Count Division Name  \\\n",
       "0       4                1                        0     Initmates   \n",
       "1       5                1                        4       General   \n",
       "\n",
       "  Department Name Class Name  \n",
       "0        Intimate  Intimates  \n",
       "1         Dresses    Dresses  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# done: load the data. \n",
    "filepath = \"/home/workspace/workspaces/full-stack-ml-metaflow-corise-week-1\"\n",
    "\n",
    "df = pd.read_csv(filepath+\"/data/Womens Clothing E-Commerce Reviews.csv\", index_col=0)\n",
    "df.head(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @futureme: update labeling fuction logic from week 1\n",
    "\n",
    "def labeling_function(row):\n",
    "    \"\"\"\n",
    "    A function to derive labels from the user's review data.\n",
    "    This could use many variables, or just one. \n",
    "    In supervised learning scenarios, this is a very important part of determining what the machine learns!\n",
    "   \n",
    "    A subset of variables in the e-commerce fashion review dataset to consider for labels you could use in ML tasks include:\n",
    "        # rating: Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\n",
    "        # recommended_ind: Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n",
    "        # positive_feedback_count: Positive Integer documenting the number of other customers who found this review positive.\n",
    "\n",
    "    In this case, we are doing sentiment analysis. \n",
    "    To keep things simple, we use the rating only, and return a binary positive or negative sentiment score based on an arbitrarty cutoff. \n",
    "    \"\"\"\n",
    "    # Done (v0): Add your logic for the labelling function here\n",
    "    # It is up to you on what value to choose as the cut off point for the postive class\n",
    "    # A good value to start would be 4\n",
    "    # This function should return either a 0 or 1 depending on the rating of a particular row\n",
    "    \n",
    "    # Try catching some invalid values\n",
    "    # if row.rating.dtype not in ('int64', 'float64'):\n",
    "    #     return 0\n",
    "    \n",
    "    if row.rating >= 4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20176/4253732853.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review'] = df['review_text'].astype('str')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>clothing_id</th>\n",
       "      <th>age</th>\n",
       "      <th>title</th>\n",
       "      <th>review_text</th>\n",
       "      <th>rating</th>\n",
       "      <th>recommended_ind</th>\n",
       "      <th>positive_feedback_count</th>\n",
       "      <th>division_name</th>\n",
       "      <th>department_name</th>\n",
       "      <th>class_name</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Initmates</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>Intimates</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  clothing_id  age                    title  \\\n",
       "0      1          767   33                      NaN   \n",
       "1      1         1080   34                      NaN   \n",
       "2      0         1077   60  Some major design flaws   \n",
       "3      1         1049   50         My favorite buy!   \n",
       "4      1          847   47         Flattering shirt   \n",
       "\n",
       "                                         review_text  rating  recommended_ind  \\\n",
       "0  Absolutely wonderful - silky and sexy and comf...       4                1   \n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
       "2  I had such high hopes for this dress and reall...       3                0   \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "4  This shirt is very flattering to all due to th...       5                1   \n",
       "\n",
       "   positive_feedback_count   division_name department_name class_name  \\\n",
       "0                        0       Initmates        Intimate  Intimates   \n",
       "1                        4         General         Dresses    Dresses   \n",
       "2                        0         General         Dresses    Dresses   \n",
       "3                        0  General Petite         Bottoms      Pants   \n",
       "4                        6         General            Tops    Blouses   \n",
       "\n",
       "                                              review  \n",
       "0  Absolutely wonderful - silky and sexy and comf...  \n",
       "1  Love this dress!  it's sooo pretty.  i happene...  \n",
       "2  I had such high hopes for this dress and reall...  \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...  \n",
       "4  This shirt is very flattering to all due to th...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformations\n",
    "df.columns = [\"_\".join(name.lower().strip().split()) for name in df.columns]\n",
    "df = df[~df.review_text.isna()]\n",
    "df['review'] = df['review_text'].astype('str')\n",
    "_has_review_df = df[df['review_text'] != 'nan']\n",
    "reviews = _has_review_df['review_text']\n",
    "labels = _has_review_df.apply(labeling_function, axis=1)\n",
    "df = pd.DataFrame({'label': labels, **_has_review_df})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of rows in train set: 18112\n",
      "num of rows in validation set: 2717\n",
      "num of rows in test set: 1812\n"
     ]
    }
   ],
   "source": [
    "# Testing the code to be deployed in FlowSpec\n",
    "\n",
    "# load libraries \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import MinMaxScalar\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.metrics as metric\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\"Split the data into training (fit), validation (trial run), test (final exam) sets\"\n",
    "SEED = 89\n",
    "split_size = 0.2\n",
    "_test_ratio = 0.4 # Default to 40% of the non-training data to be test set (train:val:test == 0.8:0.12:0.08)\n",
    "\n",
    "def train_validation_test_split (\n",
    "    X, y, train_ratio: float, validation_ratio: float, test_ratio: float\n",
    "):\n",
    "    # Split up dataset into train and test, of which we split up the test.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=(1- train_ratio), random_state=SEED\n",
    "    )\n",
    "\n",
    "    # Split up test into two (validation and test).\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_test, y_test, test_size=(test_ratio / (test_ratio + validation_ratio)), random_state=SEED,\n",
    "    )\n",
    "\n",
    "    # Return the splits\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# features (X), label (y)\n",
    "X = df.iloc[:, ~df.columns.isin(['label'])]\n",
    "y = df[['label']]\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(\n",
    "    X, y, 1.0-split_size, (1.0-_test_ratio)*(1.0-split_size), _test_ratio*(1.0-split_size)\n",
    ")\n",
    "\n",
    "print(f'num of rows in train set: {X_train.shape[0]}')\n",
    "print(f'num of rows in validation set: {X_val.shape[0]}')\n",
    "print(f'num of rows in test set: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20968</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10710</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16833</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "2661       0\n",
       "2091       0\n",
       "20968      0\n",
       "10710      1\n",
       "16833      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate from repo\n",
    "# _df = pd.DataFrame({'review': reviews, 'label': labels})\n",
    "# traindf, valdf = train_test_split(_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model accuracy:  0.7715878975265018\n",
      "Baseline AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "# TODO: build the dummy classfier baseline model.\n",
    "\n",
    "#  Next step\n",
    "\"Numerical features\"\n",
    "_num_features_ls = X_train.select_dtypes(include=['int64', 'float64']).columns.values\n",
    "\"Categorical features\"\n",
    "_cat_features_ls = X_train.select_dtypes(include='category').columns.values\n",
    "\n",
    "\"Compute the baseline\"\n",
    "# Basic baseline can be as simple as predicting the most frequent class for a classification problem\n",
    "# _pct_positive_sentiment = self.traindf.labels.sum() / self.traindf.labels.shape[0]\n",
    "\n",
    "dummy_model = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_model.fit(X_train, y_train)\n",
    "\n",
    "model = dict()\n",
    "model[0] = dummy_model\n",
    "\n",
    "### Done: Fit and score a baseline model on the data, log the acc and rocauc as artifacts.\n",
    "base_acc = dummy_model.score(X_train, y_train)\n",
    "base_rocauc = 0.5 # AUC will be 0.5 for a dummy or random baseline model\n",
    "\n",
    "dummy_y_predict = dummy_model.predict(X_train)\n",
    "# base_acc_1 = accuracy_score(y_train, dummy_y_predict)\n",
    "# base_rocauc_1 = roc_auc_score(y_train, dummy_y_predict)\n",
    "\n",
    "print(f'Baseline model accuracy:  {base_acc}')\n",
    "print(f'Baseline AUC: {base_rocauc}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find the majority class in the labels. ü§î\n",
    "# TODO: score the model on valdf with a 2D metric space: sklearn.metrics.accuracy_score, sklearn.metrics.roc_auc_score\n",
    "    # Documentation on suggested model-scoring approach: https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers, regularizers\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "\n",
    "class NbowModel():\n",
    "    def __init__(self, vocab_sz):\n",
    "\n",
    "        self.vocab_sz = vocab_sz\n",
    "\n",
    "        # Instantiate the CountVectorizer\n",
    "        self.cv = CountVectorizer(\n",
    "            min_df=.0025, max_df = .5, stop_words='english', # reduce min freq to 0.25%, and max freq to 50%.\n",
    "            strip_accents='ascii', max_features=self.vocab_sz\n",
    "        )\n",
    "\n",
    "        # Define the keras model\n",
    "        # boilerplate from week-2 below\n",
    "        # inputs = tf.keras.Input(shape=(self.vocab_sz,), \n",
    "        #                         name='input')\n",
    "        # x = layers.Dropout(0.10)(inputs)\n",
    "        # x = layers.Dense(\n",
    "        #     15, activation=\"relu\",\n",
    "        #     kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)\n",
    "        # )(x)\n",
    "        # predictions = layers.Dense(1, activation=\"sigmoid\",)(x)\n",
    "        # self.model = tf.keras.Model(inputs, predictions)\n",
    "        # opt = optimizers.Adam(learning_rate=0.002)\n",
    "        # self.model.compile(loss=\"binary_crossentropy\", \n",
    "        #                    optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "        # week-2 mucking around with keras model\n",
    "        self.model = tf.keras.Sequential([\n",
    "                    layers.Dense(10, input_shape=(self.vocab_sz,), activation=None, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)),\n",
    "                    layers.LeakyReLU(alpha=0.1),\n",
    "                    Dense(1, activation='sigmoid')\n",
    "                ])\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(X.shape)\n",
    "        print(X[0])\n",
    "        res = self.cv.fit_transform(X).toarray()\n",
    "        self.model.fit(x=res, y=y, batch_size=32, \n",
    "                       epochs=10, validation_split=.2)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        print(X.shape)\n",
    "        print(X[0])\n",
    "        res = self.cv.transform(X).toarray()\n",
    "        return self.model.predict(res)\n",
    "    \n",
    "    def eval_acc(self, X, labels, threshold=.5):\n",
    "        return accuracy_score(labels, \n",
    "                              self.predict(X) > threshold)\n",
    "    \n",
    "    def eval_rocauc(self, X, labels):\n",
    "        return roc_auc_score(labels,  self.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18112,)\n",
      "Absolutely wonderful - silky and sexy and comfortable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 19:54:11.201375: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-05-11 19:54:11.220891: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2499995000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.4894 - accuracy: 0.7681 - val_loss: 0.3331 - val_accuracy: 0.8592\n",
      "Epoch 2/10\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.3114 - accuracy: 0.8724 - val_loss: 0.3298 - val_accuracy: 0.8595\n",
      "Epoch 3/10\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.2830 - accuracy: 0.8848 - val_loss: 0.3291 - val_accuracy: 0.8636\n",
      "Epoch 4/10\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.2817 - accuracy: 0.8865 - val_loss: 0.3297 - val_accuracy: 0.8590\n",
      "Epoch 5/10\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.2735 - accuracy: 0.8898 - val_loss: 0.3295 - val_accuracy: 0.8617\n",
      "Epoch 6/10\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.2753 - accuracy: 0.8867 - val_loss: 0.3309 - val_accuracy: 0.8614\n",
      "Epoch 7/10\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.2649 - accuracy: 0.8930 - val_loss: 0.3331 - val_accuracy: 0.8581\n",
      "Epoch 8/10\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.2696 - accuracy: 0.8904 - val_loss: 0.3350 - val_accuracy: 0.8625\n",
      "Epoch 9/10\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.2578 - accuracy: 0.8965 - val_loss: 0.3369 - val_accuracy: 0.8562\n",
      "Epoch 10/10\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 0.2539 - accuracy: 0.9022 - val_loss: 0.3387 - val_accuracy: 0.8598\n"
     ]
    }
   ],
   "source": [
    "# Experimentation phase: fit the custom model on data\n",
    "model_test = NbowModel(vocab_sz=750)\n",
    "model_test.fit(X=X_train[\"review\"], y=y_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val['label'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18112,)\n",
      "Absolutely wonderful - silky and sexy and comfortable\n",
      "(18112,)\n",
      "Absolutely wonderful - silky and sexy and comfortable\n",
      "Baseline Accuracy: 0.899\n",
      "Baseline AUC: 0.944\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the exp. model perf (nlp-2.ipynb tutorial: https://outerbounds.com/docs/nlp-tutorial-L2/)\n",
    "\n",
    "# valdf = pd.read_parquet('valid.parquet')\n",
    "model_acc = model_test.eval_acc(\n",
    "    X=X_train['review'], labels=y_train['label'])\n",
    "model_rocauc = model_test.eval_rocauc(\n",
    "    X=X_train['review'], labels=y_train['label'])\n",
    "\n",
    "msg = 'Baseline Accuracy: {}\\nBaseline AUC: {}'\n",
    "print(msg.format(\n",
    "    round(model_acc, 3), round(model_rocauc, 3)\n",
    "))\n",
    "\n",
    "# Having some issues with calling model.eval_acc() with X_val and y_val --> will debug later\n",
    "# still don't understand why there is something being printed out ((18112,)\n",
    "# Absolutely wonderful - silky and sexy and comfortable\n",
    "# (18112,)\n",
    "# Absolutely wonderful - silky and sexy and comfortable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "# done: modify this custom model to your liking. Check out this tutorial for more on this class: https://outerbounds.com/docs/nlp-tutorial-L2/\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers, regularizers\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "\n",
    "class NbowModel():\n",
    "    def __init__(self, vocab_sz):\n",
    "\n",
    "        self.vocab_sz = vocab_sz\n",
    "\n",
    "        # Instantiate the CountVectorizer\n",
    "        self.cv = CountVectorizer(\n",
    "            min_df=.0025, max_df = .5, stop_words='english', # reduce min freq to 0.25%, and max freq to 50%.\n",
    "            strip_accents='ascii', max_features=self.vocab_sz\n",
    "        )\n",
    "\n",
    "        # Define the keras model\n",
    "        # boilerplate from week-2 below\n",
    "        inputs = tf.keras.Input(shape=(self.vocab_sz,), \n",
    "                                name='input')\n",
    "        x = layers.Dropout(0.10)(inputs)\n",
    "        x = layers.Dense(\n",
    "            15, activation=\"relu\",\n",
    "            kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)\n",
    "        )(x)\n",
    "        predictions = layers.Dense(1, activation=\"sigmoid\",)(x)\n",
    "        self.model = tf.keras.Model(inputs, predictions)\n",
    "        opt = optimizers.Adam(learning_rate=0.002)\n",
    "        self.model.compile(loss=\"binary_crossentropy\", \n",
    "                           optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "        # week-2 mucking around with keras model\n",
    "        # self.model = tf.keras.Sequential([\n",
    "        #             layers.Dense(10, input_shape=(self.vocab_sz,), activation=None, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)),\n",
    "        #             layers.LeakyReLU(alpha=0.1),\n",
    "        #             Dense(1, activation='sigmoid')\n",
    "        #         ])\n",
    "        \n",
    "        # week-2 compile the model\n",
    "        # self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(X.shape)\n",
    "        print(X[0])\n",
    "        res = self.cv.fit_transform(X).toarray()\n",
    "        self.model.fit(x=res, y=y, batch_size=32, \n",
    "                       epochs=10, validation_split=.2)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        print(X.shape)\n",
    "        print(X[0])\n",
    "        res = self.cv.transform(X).toarray()\n",
    "        return self.model.predict(res)\n",
    "    \n",
    "    def eval_acc(self, X, labels, threshold=.5):\n",
    "        return accuracy_score(labels, \n",
    "                              self.predict(X) > threshold)\n",
    "    \n",
    "    def eval_rocauc(self, X, labels):\n",
    "        return roc_auc_score(labels,  self.predict(X))\n",
    "\n",
    "    @property\n",
    "    def model_dict(self): \n",
    "        return {'vectorizer':self.cv, 'model': self.model}\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, model_dict):\n",
    "        \"Get Model from dictionary\"\n",
    "        nbow_model = cls(len(\n",
    "            model_dict['vectorizer'].vocabulary_\n",
    "        ))\n",
    "        nbow_model.model = model_dict['model']\n",
    "        nbow_model.cv = model_dict['vectorizer']\n",
    "        return nbow_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train the model on traindf.\n",
    "# TODO: score the model on valdf with _the same_ 2D metric space you used in previous cell.\n",
    "# TODO: test your model works by importing the model module in notebook cells, and trying to fit traindf and score predictions on the valdf data!\n",
    "\n",
    "# # from model import NbowModel\n",
    "# model = NbowModel(vocab_sz=750)\n",
    "# model.fit(X=X_train[\"review\"], y=y_train[\"label\"])\n",
    "# self.model_dict = model.model_dict  # save model\n",
    "# self.next(self.join)\n",
    "\n",
    "# model_acc = model.eval_acc(\n",
    "#     valdf['review'], valdf['labels'])\n",
    "# model_rocauc = model.eval_rocauc(\n",
    "#     valdf['review'], valdf['labels'])\n",
    "\n",
    "# msg = 'Baseline Accuracy: {}\\nBaseline AUC: {}'\n",
    "# print(msg.format(\n",
    "#     round(model_acc, 3), round(model_rocauc, 3)\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import IncludeFile\n",
    "data = IncludeFile('data', default='Womens Clothing E-Commerce Reviews.csv')\n",
    "\n",
    "# _df = pd.read_csv(data)\n",
    "\n",
    "# example:\n",
    "# from metaflow import FlowSpec, IncludeFile\n",
    "# import pandas as pd\n",
    "\n",
    "# class MyFlow(FlowSpec):\n",
    "\n",
    "#     # Include the CSV file in the flow\n",
    "#     data_file = IncludeFile('my_data', default='my_data.csv')\n",
    "\n",
    "#     def step1(self):\n",
    "#         # Read the CSV file into a pandas DataFrame\n",
    "#         df = pd.read_csv(self.data_file)\n",
    "#         print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of rows in train set: 18112\n",
      "num of rows in validation set: 2717\n",
      "num of rows in test set: 1812\n"
     ]
    }
   ],
   "source": [
    "# Let's try the baseline challenge\n",
    "\n",
    "# split_size = Parameter('split-sz', default=0.2)\n",
    "# data = IncludeFile('data', default='Womens Clothing E-Commerce Reviews.csv')\n",
    "# kfold = Parameter('k', default=5)\n",
    "# scoring = Parameter('scoring', default='accuracy')\n",
    "\n",
    "labeling_function =  lambda x: 1 if x.rating >=4 else 0 # done: Define your labeling function here.\n",
    "# def @step first step is to read the data from wherever and clean and save it as an artifact\n",
    "\n",
    "import pandas as pd\n",
    "import io \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load dataset packaged with the flow.\n",
    "# this technique is convenient when working with small datasets that need to move to remove tasks.\n",
    "_df_raw = df.copy() #for ipynb\n",
    "# done: load the data in flowspec. \n",
    "# df = pd.read_csv(self.data)\n",
    "\n",
    "\n",
    "# filter down to reviews and labels \n",
    "_df_raw.columns = [\"_\".join(name.lower().strip().split()) for name in _df_raw.columns]\n",
    "_df_raw = _df_raw[~_df_raw.review_text.isna()]\n",
    "\n",
    "_df_raw['review'] = _df_raw['review_text'].astype('str')\n",
    "_has_review_df = _df_raw[_df_raw['review_text'] != 'nan']\n",
    "\n",
    "reviews = _has_review_df['review_text']\n",
    "labels = _has_review_df.apply(labeling_function, axis=1)\n",
    "_df_clean = pd.DataFrame({'label': labels, **_has_review_df})\n",
    "\n",
    "# split the data 80/20, or by using the flow's split-sz CLI argument\n",
    "_df = pd.DataFrame({'review': reviews, 'label': labels})\n",
    "\n",
    "# features (X), label (y)\n",
    "X = _df.iloc[:, ~_df.columns.isin(['label'])]\n",
    "y = _df[['label']]\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(\n",
    "    X, y, 1.0-split_size, (1.0-_test_ratio)*(1.0-split_size), _test_ratio*(1.0-split_size)\n",
    ")\n",
    "\n",
    "print(f'num of rows in train set: {X_train.shape[0]}')\n",
    "print(f'num of rows in validation set: {X_val.shape[0]}')\n",
    "print(f'num of rows in test set: {X_test.shape[0]}')\n",
    "\n",
    "# self.traindf, self.valdf = train_test_split(_df, test_size=self.split_size)\n",
    "# print(f'num of rows in train set: {self.traindf.shape[0]}')\n",
    "# print(f'num of rows in validation set: {self.valdf.shape[0]}')\n",
    "\n",
    "# self.next(self.baseline, self.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to create a picklable model object to serialize and store as artifact in order to deserialize later on\n",
    "# from dataclasses import dataclass, asdict, fields\n",
    "# self.result_serialized = asdict(ModelResult(\"Baseline\", params, pathspec, acc, rocauc))\n",
    "\n",
    "@dataclass\n",
    "class ModelResult:\n",
    "    \"A custom struct for storing model evaluation results.\"\n",
    "    name: None\n",
    "    params: None\n",
    "    pathspec: None\n",
    "    acc: None\n",
    "    rocauc: None\n",
    "\n",
    "# def classFromArgs(className, argDict):\n",
    "#     fieldSet = {f.name for f in fields(className) if f.init}\n",
    "#     filteredArgDict = {k : v for k, v in argDict.items() if k in fieldSet}\n",
    "#     return className(**filteredArgDict)\n",
    "\n",
    "# data = Task('BaselineChallenge/87/baseline/428')['result_serialized'].data \n",
    "# print(classFromArgs(ModelResult, data))\n",
    "# output:\n",
    "# ModelResult(name='Baseline', params='Always predict 1', pathspec='BaselineChallenge/87/baseline/428', acc=0.7697063369397218, rocauc=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the majority label is 1\n",
      "(22641,)\n",
      "Absolutely wonderful - silky and sexy and comfortable\n",
      "Accuracy of majority class model is 79.77%, AUC of 0.5\n"
     ]
    }
   ],
   "source": [
    "# basline step\n",
    "\n",
    "\"Compute the baseline\"\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# these 3 arguments are needed to instantiate an object of the ModelResult class\n",
    "# self._name = \"baseline\"\n",
    "# params = \"Always predict 1\"\n",
    "# pathspec = f\"{current.flow_name}/{current.run_id}/{current.step_name}/{current.task_id}\"\n",
    "\n",
    "def getMajorityClass(df, col_label):\n",
    "    return df[col_label].mode()[0]\n",
    "\n",
    "predictions = getMajorityClass(_df, 'label') # done: predict the majority class\n",
    "print(f'the majority label is {predictions}')\n",
    "\n",
    "acc = model_test.eval_acc(\n",
    "    X=_df['review'], \n",
    "    labels=pd.DataFrame({'label': [predictions] * _df.shape[0]})\n",
    "    ) # done: return the accuracy_score of these predictions    \n",
    "\n",
    "# ideally use the defined method with the model class for rocauc calc but Im getting an understandable error\n",
    "# \"ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\"\n",
    "# rocauc = model_test.eval_rocauc(\n",
    "#     X=_df['review'], \n",
    "#     labels=pd.DataFrame({'label': [predictions] * _df.shape[0]})\n",
    "#  ) # done: return the roc_auc_score of these predictions\n",
    "\n",
    "# People have bypassed this with below but using the y_val dataset always\n",
    "rocauc = roc_auc_score(_df[\"label\"],pd.DataFrame({'label': [predictions] * _df.shape[0]}))\n",
    "\n",
    "print(f'Accuracy of majority class model is {round(acc*100, 2)}%, AUC of {rocauc}')\n",
    "\n",
    "# self.result = ModelResult(\"Baseline\", params, pathspec, acc, rocauc)\n",
    "# self.next(self.aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18112,)\n",
      "Absolutely wonderful - silky and sexy and comfortable\n",
      "Epoch 1/10\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.5270 - accuracy: 0.7329 - val_loss: 0.4032 - val_accuracy: 0.8109\n",
      "Epoch 2/10\n",
      "453/453 [==============================] - 0s 882us/step - loss: 0.4052 - accuracy: 0.8180 - val_loss: 0.4004 - val_accuracy: 0.8153\n",
      "Epoch 3/10\n",
      "453/453 [==============================] - 0s 879us/step - loss: 0.3928 - accuracy: 0.8221 - val_loss: 0.3998 - val_accuracy: 0.8156\n",
      "Epoch 4/10\n",
      "453/453 [==============================] - 0s 894us/step - loss: 0.3808 - accuracy: 0.8329 - val_loss: 0.3991 - val_accuracy: 0.8181\n",
      "Epoch 5/10\n",
      "453/453 [==============================] - 0s 912us/step - loss: 0.3801 - accuracy: 0.8337 - val_loss: 0.3976 - val_accuracy: 0.8222\n",
      "Epoch 6/10\n",
      "453/453 [==============================] - 0s 936us/step - loss: 0.3748 - accuracy: 0.8384 - val_loss: 0.3980 - val_accuracy: 0.8239\n",
      "Epoch 7/10\n",
      "453/453 [==============================] - 0s 916us/step - loss: 0.3607 - accuracy: 0.8478 - val_loss: 0.4003 - val_accuracy: 0.8242\n",
      "Epoch 8/10\n",
      "453/453 [==============================] - 0s 896us/step - loss: 0.3634 - accuracy: 0.8431 - val_loss: 0.4034 - val_accuracy: 0.8206\n",
      "Epoch 9/10\n",
      "453/453 [==============================] - 0s 883us/step - loss: 0.3604 - accuracy: 0.8441 - val_loss: 0.4018 - val_accuracy: 0.8256\n",
      "Epoch 10/10\n",
      "453/453 [==============================] - 0s 874us/step - loss: 0.3536 - accuracy: 0.8527 - val_loss: 0.4047 - val_accuracy: 0.8250\n",
      "(1812,)\n",
      "(18112,)\n",
      "Absolutely wonderful - silky and sexy and comfortable\n",
      "Epoch 1/10\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.5013 - accuracy: 0.7595 - val_loss: 0.4030 - val_accuracy: 0.8156\n",
      "Epoch 2/10\n",
      "453/453 [==============================] - 0s 883us/step - loss: 0.3974 - accuracy: 0.8226 - val_loss: 0.4014 - val_accuracy: 0.8222\n",
      "Epoch 3/10\n",
      "453/453 [==============================] - 0s 888us/step - loss: 0.3797 - accuracy: 0.8286 - val_loss: 0.3988 - val_accuracy: 0.8203\n",
      "Epoch 4/10\n",
      "453/453 [==============================] - 0s 878us/step - loss: 0.3849 - accuracy: 0.8297 - val_loss: 0.3963 - val_accuracy: 0.8222\n",
      "Epoch 5/10\n",
      "453/453 [==============================] - 0s 870us/step - loss: 0.3850 - accuracy: 0.8298 - val_loss: 0.3975 - val_accuracy: 0.8242\n",
      "Epoch 6/10\n",
      "453/453 [==============================] - 0s 861us/step - loss: 0.3726 - accuracy: 0.8369 - val_loss: 0.3989 - val_accuracy: 0.8206\n",
      "Epoch 7/10\n",
      "453/453 [==============================] - 0s 874us/step - loss: 0.3676 - accuracy: 0.8434 - val_loss: 0.3977 - val_accuracy: 0.8261\n",
      "Epoch 8/10\n",
      "453/453 [==============================] - 0s 904us/step - loss: 0.3705 - accuracy: 0.8411 - val_loss: 0.4002 - val_accuracy: 0.8242\n",
      "Epoch 9/10\n",
      "453/453 [==============================] - 0s 895us/step - loss: 0.3651 - accuracy: 0.8453 - val_loss: 0.4050 - val_accuracy: 0.8203\n",
      "Epoch 10/10\n",
      "453/453 [==============================] - 0s 902us/step - loss: 0.3577 - accuracy: 0.8464 - val_loss: 0.4036 - val_accuracy: 0.8275\n",
      "(1812,)\n",
      "(18112,)\n",
      "Absolutely wonderful - silky and sexy and comfortable\n",
      "Epoch 1/10\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 0.5167 - accuracy: 0.7542 - val_loss: 0.4007 - val_accuracy: 0.8203\n",
      "Epoch 2/10\n",
      "453/453 [==============================] - 0s 885us/step - loss: 0.3963 - accuracy: 0.8235 - val_loss: 0.4016 - val_accuracy: 0.8189\n",
      "Epoch 3/10\n",
      "453/453 [==============================] - 0s 877us/step - loss: 0.3894 - accuracy: 0.8265 - val_loss: 0.3984 - val_accuracy: 0.8200\n",
      "Epoch 4/10\n",
      "453/453 [==============================] - 0s 878us/step - loss: 0.3878 - accuracy: 0.8273 - val_loss: 0.3979 - val_accuracy: 0.8222\n",
      "Epoch 5/10\n",
      "453/453 [==============================] - 0s 900us/step - loss: 0.3776 - accuracy: 0.8308 - val_loss: 0.3972 - val_accuracy: 0.8228\n",
      "Epoch 6/10\n",
      "453/453 [==============================] - 0s 897us/step - loss: 0.3804 - accuracy: 0.8350 - val_loss: 0.4010 - val_accuracy: 0.8222\n",
      "Epoch 7/10\n",
      "453/453 [==============================] - 0s 867us/step - loss: 0.3682 - accuracy: 0.8378 - val_loss: 0.3985 - val_accuracy: 0.8217\n",
      "Epoch 8/10\n",
      "453/453 [==============================] - 0s 892us/step - loss: 0.3657 - accuracy: 0.8426 - val_loss: 0.4000 - val_accuracy: 0.8217\n",
      "Epoch 9/10\n",
      "453/453 [==============================] - 0s 891us/step - loss: 0.3568 - accuracy: 0.8485 - val_loss: 0.4007 - val_accuracy: 0.8256\n",
      "Epoch 10/10\n",
      "453/453 [==============================] - 0s 891us/step - loss: 0.3653 - accuracy: 0.8443 - val_loss: 0.4020 - val_accuracy: 0.8220\n",
      "(1812,)\n"
     ]
    }
   ],
   "source": [
    "# Testing the results from hyperperameter search\n",
    "\n",
    "# def model(self):\n",
    "\n",
    "# TODO: import your model if it is defined in another file.\n",
    "from model import NbowModel\n",
    "import pandas as pd\n",
    "# self._name = \"model\"\n",
    "\n",
    "# NOTE: If you followed the link above to find a custom model implementation, \n",
    "    # you will have noticed your model's vocab_sz hyperparameter.\n",
    "    # Too big of vocab_sz causes an error. Can you explain why? \n",
    "# self.hyperparam_set = [{'vocab_sz': 100}, {'vocab_sz': 300}, {'vocab_sz': 500}]  \n",
    "hyperparam_set = [{'vocab_sz': 100}, {'vocab_sz': 300}, {'vocab_sz': 500}]  \n",
    "# pathspec = f\"{current.flow_name}/{current.run_id}/{current.step_name}/{current.task_id}\"\n",
    "pathspec = f\"/home/workspace/workspaces/full-stack-ml-metaflow-corise-week-2/data\"\n",
    "\n",
    "# self.results = []\n",
    "results = []\n",
    "for params in hyperparam_set: #self.hyperparam_set\n",
    "    model = NbowModel(vocab_sz=150) # done: instantiate your custom model here!\n",
    "    model.fit(X=X_train['review'], y=y_train['label'])\n",
    "    try:\n",
    "        acc = model.eval_acc(X_test['review'], y_test['label'])# TODO: evaluate your custom model in an equivalent way to accuracy_score.\n",
    "        rocauc = model_rocauc = model.eval_rocauc(X_test['review'], y_test['label']) # TODO: evaluate your custom model in an equivalent way to roc_auc_score.\n",
    "        results.append(ModelResult(f\"NbowModel - vocab_sz: {params['vocab_sz']}\", params, pathspec, acc, rocauc))\n",
    "    except KeyError:\n",
    "        results.append(ModelResult(f\"NbowModel - vocab_sz: {params['vocab_sz']}\", params, pathspec, None, None))\n",
    "# self.next(self.aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting baseline_challenge.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile baseline_challenge.py\n",
    "# in progress: In this cell, write your BaselineChallenge flow in the baseline_challenge.py file.\n",
    "\n",
    "from metaflow import FlowSpec, step, Flow, current, Parameter, IncludeFile, card, current\n",
    "from metaflow.cards import Table, Markdown, Artifact, Image\n",
    "import numpy as np \n",
    "from dataclasses import dataclass\n",
    "\n",
    "labeling_function =  lambda x: 1 if x.rating >=4 else 0 # done: Define your labeling function here.\n",
    "\n",
    "@dataclass\n",
    "class ModelResult:\n",
    "    \"A custom struct for storing model evaluation results.\"\n",
    "    name: None\n",
    "    params: None\n",
    "    pathspec: None\n",
    "    acc: None\n",
    "    rocauc: None\n",
    "\n",
    "class BaselineChallenge(FlowSpec):\n",
    "\n",
    "    split_size = Parameter('split-sz', default=0.2)\n",
    "    _test_ratio = 0.4\n",
    "    data = IncludeFile('data', default='Womens Clothing E-Commerce Reviews.csv')\n",
    "    kfold = Parameter('k', default=5)\n",
    "    scoring = Parameter('scoring', default='accuracy')\n",
    "\n",
    "    @step\n",
    "    def start(self):\n",
    "\n",
    "        import pandas as pd\n",
    "        import io \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # load dataset packaged with the flow.\n",
    "        # this technique is convenient when working with small datasets that need to move to remove tasks.\n",
    "        # done: load the data in flowspec. \n",
    "        _df_raw = pd.read_csv(self.data)\n",
    "\n",
    "\n",
    "        # filter down to reviews and labels \n",
    "        _df_raw.columns = [\"_\".join(name.lower().strip().split()) for name in _df_raw.columns]\n",
    "        _df_raw = _df_raw[~_df_raw.review_text.isna()]\n",
    "\n",
    "        _df_raw['review'] = _df_raw['review_text'].astype('str')\n",
    "        _has_review_df = _df_raw[_df_raw['review_text'] != 'nan']\n",
    "\n",
    "        reviews = _has_review_df['review_text']\n",
    "        labels = _has_review_df.apply(labeling_function, axis=1)\n",
    "        _df_clean = pd.DataFrame({'label': labels, **_has_review_df})\n",
    "\n",
    "        # split the data 80/20, or by using the flow's split-sz CLI argument\n",
    "        _df = pd.DataFrame({'review': reviews, 'label': labels})\n",
    "\n",
    "        # features (X), label (y)\n",
    "        X = _df.iloc[:, ~_df.columns.isin(['label'])]\n",
    "        y = _df[['label']]\n",
    "\n",
    "        \"Split the data into training (fit), validation (trial run), test (final exam) sets\"\n",
    "        SEED = 89\n",
    "\n",
    "\n",
    "        def train_validation_test_split (\n",
    "            X, y, train_ratio: float, validation_ratio: float, test_ratio: float\n",
    "        ):\n",
    "            # Split up dataset into train and test, of which we split up the test.\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=(1- train_ratio), random_state=SEED\n",
    "            )\n",
    "\n",
    "            # Split up test into two (validation and test).\n",
    "            X_val, X_test, y_val, y_test = train_test_split(\n",
    "            X_test, y_test, test_size=(test_ratio / (test_ratio + validation_ratio)), random_state=SEED,\n",
    "            )\n",
    "\n",
    "            # Return the splits\n",
    "            return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "            \n",
    "        # split the data 80/20, or by using the flow's split-sz CLI argument\n",
    "        self.X_train, self.X_val, self.X_test, self.y_train, self.y_val, self.y_test = train_validation_test_split(\n",
    "            X, y, 1.0-split_size, (1.0-_test_ratio)*(1.0-split_size), _test_ratio*(1.0-split_size)\n",
    "        )\n",
    "        \n",
    "        print(f'num of rows in train set: {self.X_train.shape[0]}')\n",
    "        print(f'num of rows in validation set: {self.X_val.shape[0]}')\n",
    "\n",
    "        self.next(self.baseline, self.model)\n",
    "\n",
    "    @step\n",
    "    def baseline(self):\n",
    "        \"Compute the baseline\"\n",
    "\n",
    "        from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "        import pandas as pd\n",
    "        \n",
    "        self._name = \"baseline\"\n",
    "        params = \"Always predict 1\"\n",
    "        pathspec = f\"{current.flow_name}/{current.run_id}/{current.step_name}/{current.task_id}\"\n",
    "        def getMajorityClass(df, col_label):\n",
    "            return df[col_label].mode()[0]\n",
    "\n",
    "        predictions = getMajorityClass(_df, 'label') # done: predict the majority class\n",
    "        # print(f'the majority label is {predictions}')\n",
    "\n",
    "        acc = model_test.eval_acc(\n",
    "            X=self.X_train['review'], \n",
    "            labels=pd.DataFrame({'label': [predictions] * self.X_train.shape[0]})\n",
    "            ) # done: return the accuracy_score of these predictions    \n",
    "\n",
    "        # ideally use the defined method with the model class for rocauc calc but Im getting an understandable error\n",
    "        # \"ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\"\n",
    "        # rocauc = model_test.eval_rocauc(\n",
    "        #     X=_df['review'], \n",
    "        #     labels=pd.DataFrame({'label': [predictions] * _df.shape[0]})\n",
    "        #  ) # done: return the roc_auc_score of these predictions\n",
    "\n",
    "        # People have bypassed this with below but using the y_val dataset always\n",
    "        rocauc = roc_auc_score(self.X_train[\"label\"],pd.DataFrame({'label': [predictions] * self.X_train.shape[0]}))\n",
    "        # print(f'Accuracy of majority class model is {round(acc*100, 2)}%, AUC of {rocauc}')\n",
    "\n",
    "        self.result = ModelResult(\"Baseline\", params, pathspec, acc, rocauc)\n",
    "        self.next(self.aggregate)\n",
    "\n",
    "    @step\n",
    "    def model(self):\n",
    "\n",
    "        # done: import your model if it is defined in another file.\n",
    "        from model import NbowModel\n",
    "        import pandas as pd\n",
    "\n",
    "        self._name = \"model\"\n",
    "        # NOTE: If you followed the link above to find a custom model implementation, \n",
    "            # you will have noticed your model's vocab_sz hyperparameter.\n",
    "            # Too big of vocab_sz causes an error. Can you explain why? \n",
    "        self.hyperparam_set = [{'vocab_sz': 100}, {'vocab_sz': 300}, {'vocab_sz': 500}]  \n",
    "        pathspec = f\"{current.flow_name}/{current.run_id}/{current.step_name}/{current.task_id}\"\n",
    "\n",
    "        self.results = []\n",
    "        for params in self.hyperparam_set:\n",
    "            model = NbowModel(vocab_sz=150) # done: instantiate your custom model here!\n",
    "            model.fit(X=self.X_train['review'], y=self.y_train['label'])\n",
    "            try:\n",
    "                acc = model.eval_acc(self.X_test['review'], self.y_test['label'])# done: evaluate your custom model in an equivalent way to accuracy_score.\n",
    "                rocauc = model.eval_rocauc(self.X_test['review'], self.y_test['label']) # done: evaluate your custom model in an equivalent way to roc_auc_score.\n",
    "                self.results.append(ModelResult(f\"NbowModel - vocab_sz: {params['vocab_sz']}\", params, pathspec, acc, rocauc))\n",
    "            except KeyError:\n",
    "                self.results.append(ModelResult(f\"NbowModel - vocab_sz: {params['vocab_sz']}\", params, pathspec, None, None))\n",
    "\n",
    "        self.next(self.aggregate)\n",
    "\n",
    "    @card(type='corise')\n",
    "    @step\n",
    "    def aggregate(self, inputs):\n",
    "\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        from matplotlib import rcParams \n",
    "        rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "        rows = []\n",
    "        violin_plot_df = {'name': [], 'accuracy': []}\n",
    "        for task in inputs:\n",
    "            if task._name == \"model\": \n",
    "                for result in task.results:\n",
    "                    print(result)\n",
    "                    rows, violin_plot_df = self.add_one(rows, result, violin_plot_df)\n",
    "            elif task._name == \"baseline\":\n",
    "                print(task.result)\n",
    "                rows, violin_plot_df = self.add_one(rows, task.result, violin_plot_df)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown task._name type. Cannot parse results.\")\n",
    "            \n",
    "        current.card.append(Markdown(\"# All models from this flow run\"))\n",
    "\n",
    "        \n",
    "        current.card.append(\n",
    "            Table(\n",
    "                rows,\n",
    "                headers=[\"Model name\", \"Params\", \"Task pathspec\", \"Accuracy\", \"ROCAUC\"]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        plt.xticks(rotation=40)\n",
    "        sns.violinplot(data=violin_plot_df, x=\"name\", y=\"accuracy\", ax=ax)\n",
    "        \n",
    "        # TODO: Append the matplotlib fig to the card\n",
    "        # Docs: https://docs.metaflow.org/metaflow/visualizing-results/easy-custom-reports-with-card-components#showing-plots\n",
    "        current.card.append(Image.from_matplotlib(fig))\n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BaselineChallenge()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Anticipating Failure in Your Machine Learning Project\n",
    "\n",
    "In this task, you'll practice anticipating potential failure modes in a sentiment analysis classifier and develop strategies to mitigate them. Here's what you'll need to do:\n",
    "### Step 1: Identify Potential Failure Modes\n",
    "\n",
    "The first step in anticipating failure in your machine learning project is to identify potential failure modes. Start by brainstorming ways in which your project could fail from an engineering point of view. For example, your model could overfit to the training data or suffer from data bias.\n",
    "### Step 2: Develop Strategies to Mitigate Failure Modes\n",
    "\n",
    "Once you've identified potential failure modes, the next step is to develop strategies to mitigate them. Think about what measures you could take to fix the issue if it were to occur. For example, if your model is overfitting to the training data, you could try regularization techniques such as L1 or L2 regularization to reduce the complexity of the model.\n",
    "### Step 3: Plan Ahead to Avoid Failure Modes\n",
    "\n",
    "Finally, it's important to plan ahead to avoid potential failure modes in the first place. Think about what you could have done initially to avoid these failure modes. For example, you could have collected a diverse set of training data to reduce data bias or experimented with different model architectures to find the best solution for your problem.\n",
    "\n",
    "The key to anticipating failure in your machine learning project is to be proactive rather than reactive. By identifying potential failure modes ahead of time and developing strategies to mitigate them, you'll be better equipped to build a successful machine learning project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Visualizing ML Results with MF Cards\n",
    "Now it is time to iterate. Extend the flow in your `baseline_challenge.py` file to include a step that aggregates all of the results from hyperparameter tuning jobs, and logs results and a data visualiation in a Metaflow card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile baseline_challenge.py\n",
    "# %%writefile baseline_challenge.py\n",
    "# TODO: In this cell, write your BaselineChallenge flow in the baseline_challenge.py file.\n",
    "\n",
    "from metaflow import FlowSpec, step, Flow, current, Parameter, IncludeFile, card, current\n",
    "from metaflow.cards import Table, Markdown, Artifact, Image\n",
    "import numpy as np \n",
    "from dataclasses import dataclass\n",
    "\n",
    "labeling_function = ... # TODO: Define your labeling function here.\n",
    "\n",
    "@dataclass\n",
    "class ModelResult:\n",
    "    \"A custom struct for storing model evaluation results.\"\n",
    "    name: None\n",
    "    params: None\n",
    "    pathspec: None\n",
    "    acc: None\n",
    "    rocauc: None\n",
    "\n",
    "class BaselineChallenge(FlowSpec):\n",
    "\n",
    "    split_size = Parameter('split-sz', default=0.2)\n",
    "    data = IncludeFile('data', default='Womens Clothing E-Commerce Reviews.csv')\n",
    "    kfold = Parameter('k', default=5)\n",
    "    scoring = Parameter('scoring', default='accuracy')\n",
    "\n",
    "    @step\n",
    "    def start(self):\n",
    "\n",
    "        import pandas as pd\n",
    "        import io \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # load dataset packaged with the flow.\n",
    "        # this technique is convenient when working with small datasets that need to move to remove tasks.\n",
    "        df = ... \n",
    "        # TODO: load the data. \n",
    "        # Look up a few lines to the IncludeFile('data', default='Womens Clothing E-Commerce Reviews.csv'). \n",
    "        # You can find documentation on IncludeFile here: https://docs.metaflow.org/scaling/data#data-in-local-files\n",
    "\n",
    "\n",
    "        # filter down to reviews and labels \n",
    "        df.columns = [\"_\".join(name.lower().strip().split()) for name in df.columns]\n",
    "        df = df[~df.review_text.isna()]\n",
    "        df['review'] = df['review_text'].astype('str')\n",
    "        _has_review_df = df[df['review_text'] != 'nan']\n",
    "        reviews = _has_review_df['review_text']\n",
    "        labels = _has_review_df.apply(labeling_function, axis=1)\n",
    "        self.df = pd.DataFrame({'label': labels, **_has_review_df})\n",
    "\n",
    "        # split the data 80/20, or by using the flow's split-sz CLI argument\n",
    "        _df = pd.DataFrame({'review': reviews, 'label': labels})\n",
    "        self.traindf, self.valdf = train_test_split(_df, test_size=self.split_size)\n",
    "        print(f'num of rows in train set: {self.traindf.shape[0]}')\n",
    "        print(f'num of rows in validation set: {self.valdf.shape[0]}')\n",
    "\n",
    "        self.next(self.baseline, self.model)\n",
    "\n",
    "    @step\n",
    "    def baseline(self):\n",
    "        \"Compute the baseline\"\n",
    "\n",
    "        from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "        self._name = \"baseline\"\n",
    "        params = \"Always predict 1\"\n",
    "        pathspec = f\"{current.flow_name}/{current.run_id}/{current.step_name}/{current.task_id}\"\n",
    "\n",
    "        predictions = ... # TODO: predict the majority class\n",
    "        acc = # TODO: return the accuracy_score of these predictions\n",
    "         \n",
    "        rocauc = # TODO: return the roc_auc_score of these predictions\n",
    "        self.result = ModelResult(\"Baseline\", params, pathspec, acc, rocauc)\n",
    "        self.next(self.aggregate)\n",
    "\n",
    "    @step\n",
    "    def model(self):\n",
    "\n",
    "        # TODO: import your model if it is defined in another file.\n",
    "\n",
    "        self._name = \"model\"\n",
    "        # NOTE: If you followed the link above to find a custom model implementation, \n",
    "            # you will have noticed your model's vocab_sz hyperparameter.\n",
    "            # Too big of vocab_sz causes an error. Can you explain why? \n",
    "        self.hyperparam_set = [{'vocab_sz': 100}, {'vocab_sz': 300}, {'vocab_sz': 500}]  \n",
    "        pathspec = f\"{current.flow_name}/{current.run_id}/{current.step_name}/{current.task_id}\"\n",
    "\n",
    "        self.results = []\n",
    "        for params in self.hyperparam_set:\n",
    "            model = .. # TODO: instantiate your custom model here!\n",
    "            model.fit(X=self.df['review'], y=self.df['label'])\n",
    "            acc = # TODO: evaluate your custom model in an equivalent way to accuracy_score.\n",
    "            rocauc = # TODO: evaluate your custom model in an equivalent way to roc_auc_score.\n",
    "            self.results.append(ModelResult(f\"NbowModel - vocab_sz: {params['vocab_sz']}\", params, pathspec, acc, rocauc))\n",
    "\n",
    "        self.next(self.aggregate)\n",
    "\n",
    "    def add_one(self, rows, result, df):\n",
    "        \"A helper function to load results.\"\n",
    "        rows.append([\n",
    "            Markdown(result.name),\n",
    "            Artifact(result.params),\n",
    "            Artifact(result.pathspec),\n",
    "            Artifact(result.acc),\n",
    "            Artifact(result.rocauc)\n",
    "        ])\n",
    "        df['name'].append(result.name)\n",
    "        df['accuracy'].append(result.acc)\n",
    "        return rows, df\n",
    "\n",
    "    @card # TODO: Set your card type to \"corise\". \n",
    "            # I wonder what other card types there are?\n",
    "            # https://docs.metaflow.org/metaflow/visualizing-results\n",
    "            # https://github.com/outerbounds/metaflow-card-altair/blob/main/altairflow.py\n",
    "    @step\n",
    "    def aggregate(self, inputs):\n",
    "\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        from matplotlib import rcParams \n",
    "        rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "        rows = []\n",
    "        violin_plot_df = {'name': [], 'accuracy': []}\n",
    "        for task in inputs:\n",
    "            if task._name == \"model\": \n",
    "                for result in task.results:\n",
    "                    print(result)\n",
    "                    rows, violin_plot_df = self.add_one(rows, result, violin_plot_df)\n",
    "            elif task._name == \"baseline\":\n",
    "                print(task.result)\n",
    "                rows, violin_plot_df = self.add_one(rows, task.result, violin_plot_df)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown task._name type. Cannot parse results.\")\n",
    "            \n",
    "        current.card.append(Markdown(\"# All models from this flow run\"))\n",
    "\n",
    "        # TODO: Add a Table of the results to your card! \n",
    "        current.card.append(\n",
    "            Table(\n",
    "                ..., # TODO: What goes here to populate the Table in the card? \n",
    "                headers=[\"Model name\", \"Params\", \"Task pathspec\", \"Accuracy\", \"ROCAUC\"]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        plt.xticks(rotation=40)\n",
    "        sns.violinplot(data=violin_plot_df, x=\"name\", y=\"accuracy\", ax=ax)\n",
    "        \n",
    "        # TODO: Append the matplotlib fig to the card\n",
    "        # Docs: https://docs.metaflow.org/metaflow/visualizing-results/easy-custom-reports-with-card-components#showing-plots\n",
    "        \n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BaselineChallenge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.8.6+ob(v1)\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mBaselineChallenge\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:sandbox\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[22m    baseline_challenge.py:80:22: E0602: Undefined variable 'split_size' (undefined-variable)\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[22m    baseline_challenge.py:80:39: E0602: Undefined variable '_test_ratio' (undefined-variable)\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[22m    baseline_challenge.py:80:57: E0602: Undefined variable 'split_size' (undefined-variable)\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[22m    baseline_challenge.py:80:70: E0602: Undefined variable '_test_ratio' (undefined-variable)\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[22m    baseline_challenge.py:80:87: E0602: Undefined variable 'split_size' (undefined-variable)\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[22m    baseline_challenge.py:101:39: E0602: Undefined variable '_df' (undefined-variable)\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[22m    baseline_challenge.py:104:14: E0602: Undefined variable 'model_test' (undefined-variable)\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[1m    Pylint is not happy\u001b[0m\u001b[22m:\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[22m    \u001b[0m\u001b[31m\u001b[1mFix Pylint warnings listed above or say --no-pylint.\u001b[0m\u001b[22m\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[22m\u001b[K\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Teeny tip in VS code to save time\n",
    "# To execute the .py files:\n",
    "# Go to the ‚ÄòFolders‚Äô tab, it automatically flags the active file in the directory of interest\n",
    "# Right click the active file, and you‚Äôll see a ‚ÄòCopy path‚Äô option\n",
    "# Then, python pasted_file_path run  into the command line\n",
    "\n",
    "! python baseline_challenge.py run --data \"/home/workspace/workspaces/full-stack-ml-metaflow-corise-week-2/data/Womens Clothing E-Commerce Reviews.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "full-stack-metaflow-corise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
